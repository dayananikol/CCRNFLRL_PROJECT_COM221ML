{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e3b65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\diana nicole\\vscodeprojects\\snake-gym\\venv\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in c:\\users\\diana nicole\\vscodeprojects\\snake-gym\\venv\\lib\\site-packages (from opencv-python) (2.2.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yattag in c:\\users\\diana nicole\\vscodeprojects\\snake-gym\\venv\\lib\\site-packages (1.16.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install yattag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0855b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DQN for SnakeGame (Full Training, 5000 episodes)\n",
    "# Metrics: Sample Efficiency, Exploration, Runtime, Convergence & Stability, Policy Behavior\n",
    "# Includes gameplay recording of best episode\n",
    "# This notebook mirrors your Q-learning notebook 1:1 in structure and reporting.\n",
    "# =============================================\n",
    "\n",
    "# -----------------------\n",
    "# 0. Imports\n",
    "# -----------------------\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, deque\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # ⚙️ Keep output clean during long runs\n",
    "\n",
    "# environment\n",
    "from snake_gym.envs import snake\n",
    "from snake_gym.envs.modules import UP, DOWN, LEFT, RIGHT\n",
    "\n",
    "# DQN libs (PyTorch)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# optional: video & html\n",
    "try:\n",
    "    import cv2\n",
    "    _HAS_CV2 = True\n",
    "except Exception:\n",
    "    _HAS_CV2 = False\n",
    "\n",
    "from yattag import Doc, indent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601f831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 1. Environment Setup\n",
    "# -----------------------\n",
    "env = snake.SnakeGame()\n",
    "n_actions = 4  # 0: UP, 1: DOWN, 2: LEFT, 3: RIGHT\n",
    "dir_to_int = {UP:0, DOWN:1, LEFT:2, RIGHT:3}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d65f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2. State Representation\n",
    "# -----------------------\n",
    "def get_state(env):\n",
    "    \"\"\"Simplified symbolic state: (dx, dy, direction)\"\"\"\n",
    "    head_x, head_y = env.snake.get_head_position()\n",
    "    apple_x, apple_y = env.apple.position\n",
    "    dx = int(np.sign(apple_x - head_x))\n",
    "    dy = int(np.sign(apple_y - head_y))\n",
    "    direction = dir_to_int[env.snake.direction]\n",
    "    return (dx, dy, direction)\n",
    "\n",
    "def state_to_array(state_tuple):\n",
    "    return np.array(state_tuple, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b873f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 3. Hyperparameters\n",
    "# -----------------------\n",
    "episodes = 5000  # ⏳ Full training run (match Q-learning)\n",
    "gamma = 0.9\n",
    "epsilon_init = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.05\n",
    "# DQN-specific\n",
    "lr = 1e-3\n",
    "batch_size = 64\n",
    "replay_capacity = 10000\n",
    "target_update_freq = 100   # update target net every 100 episodes\n",
    "checkpoint_freq = 1000     # save checkpoint every 1000 episodes\n",
    "log_freq = 100             # print every 100 episodes\n",
    "hidden_size = 64           # Option A: 64x64 MLP (kept small to match tabular complexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec1511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 4. Multi-run Setup for Convergence\n",
    "# -----------------------\n",
    "seeds = [42, 123, 999]\n",
    "all_rewards = []\n",
    "all_epsilons = []\n",
    "all_steps = []\n",
    "runtime_per_seed = []\n",
    "\n",
    "# -----------------------\n",
    "# Helpers: model + replay buffer\n",
    "# -----------------------\n",
    "class DQNNet(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden=64, output_dim=4):\n",
    "        super(DQNNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        from collections import deque\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "    def push(self, s,a,r,s2,d):\n",
    "        self.buf.append((s,a,r,s2,d))\n",
    "    def sample(self, n):\n",
    "        import random\n",
    "        batch = random.sample(self.buf, n)\n",
    "        s,a,r,s2,d = zip(*batch)\n",
    "        return (np.array(s), np.array(a), np.array(r, dtype=np.float32),\n",
    "                np.array(s2), np.array(d, dtype=np.float32))\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4565d27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 5. Training Loop\n",
    "# -----------------------\n",
    "for seed in seeds:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # initialize DQN + target\n",
    "    policy_net = DQNNet(input_dim=3, hidden=hidden_size, output_dim=n_actions).to(device)\n",
    "    target_net = DQNNet(input_dim=3, hidden=hidden_size, output_dim=n_actions).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
    "    memory = ReplayBuffer(capacity=replay_capacity)\n",
    "\n",
    "    rewards_per_episode = []\n",
    "    moving_avg_rewards = deque(maxlen=10)\n",
    "    epsilons = []\n",
    "    steps_per_episode = []\n",
    "\n",
    "    epsilon = epsilon_init\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        env.reset()\n",
    "        state = get_state(env)\n",
    "        state_arr = state_to_array(state)\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            # ε-greedy action selection\n",
    "            if random.random() < epsilon:\n",
    "                action = random.randrange(n_actions)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    st = torch.tensor(state_arr, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                    qvals = policy_net(st)\n",
    "                    action = int(torch.argmax(qvals).item())\n",
    "\n",
    "            # step environment\n",
    "            frame, reward, done, _ = env.step(action)\n",
    "            next_state = get_state(env)\n",
    "            next_state_arr = state_to_array(next_state)\n",
    "\n",
    "            # store transition\n",
    "            memory.push(state_arr, action, reward, next_state_arr, done)\n",
    "\n",
    "            state_arr = next_state_arr\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # DQN update (sample mini-batch)\n",
    "            if len(memory) >= batch_size:\n",
    "                s_batch, a_batch, r_batch, s2_batch, d_batch = memory.sample(batch_size)\n",
    "                s_t = torch.tensor(s_batch, dtype=torch.float32, device=device)\n",
    "                a_t = torch.tensor(a_batch, dtype=torch.long, device=device)\n",
    "                r_t = torch.tensor(r_batch, dtype=torch.float32, device=device)\n",
    "                s2_t = torch.tensor(s2_batch, dtype=torch.float32, device=device)\n",
    "                d_t = torch.tensor(d_batch, dtype=torch.float32, device=device)\n",
    "\n",
    "                q_values = policy_net(s_t).gather(1, a_t.unsqueeze(1)).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    next_q = target_net(s2_t).max(1)[0]\n",
    "                    target_q = r_t + gamma * next_q * (1 - d_t)\n",
    "                loss = nn.MSELoss()(q_values, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # end episode bookkeeping\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        rewards_per_episode.append(total_reward)\n",
    "        moving_avg_rewards.append(total_reward)\n",
    "        epsilons.append(epsilon)\n",
    "        steps_per_episode.append(steps)\n",
    "\n",
    "        # Show progress every log_freq episodes (100)\n",
    "        if (episode + 1) % log_freq == 0:\n",
    "            avg_reward = np.mean(moving_avg_rewards) if len(moving_avg_rewards)>0 else 0.0\n",
    "            print(f\"Seed {seed} | Episode {episode+1}/{episodes} | Avg Reward (last {len(moving_avg_rewards)}): {avg_reward:.2f} | ε={epsilon:.3f}\")\n",
    "\n",
    "        # Checkpoint every checkpoint_freq episodes (1000)\n",
    "        if (episode + 1) % checkpoint_freq == 0:\n",
    "            ckpt_path = f\"checkpoint_dqn_seed_{seed}_ep_{episode+1}.pth\"\n",
    "            torch.save(policy_net.state_dict(), ckpt_path)\n",
    "            print(f\"💾 Checkpoint saved at episode {episode+1}: {ckpt_path}\")\n",
    "\n",
    "        # Update target network periodically\n",
    "        if (episode + 1) % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    # done with seed\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_episode = total_time / episodes\n",
    "    print(f\"\\nSeed {seed} | Total Training Time: {total_time:.2f}s | Avg per episode: {avg_time_per_episode:.3f}s\\n\")\n",
    "\n",
    "    all_rewards.append(rewards_per_episode)\n",
    "    all_epsilons.append(epsilons)\n",
    "    all_steps.append(steps_per_episode)\n",
    "    runtime_per_seed.append(avg_time_per_episode)\n",
    "\n",
    "    # Save trained model (equivalent to Q-table saving)\n",
    "    model_path = f\"dqn_model_seed_{seed}.pth\"\n",
    "    torch.save(policy_net.state_dict(), model_path)\n",
    "    print(f\"✅ DQN model saved as {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ba46d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 6. Sample Efficiency\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "ma_window = 10\n",
    "for i, rewards in enumerate(all_rewards):\n",
    "    ma = np.convolve(rewards, np.ones(ma_window)/ma_window, mode='valid') if len(rewards)>=ma_window else np.array([])\n",
    "    max_reward = max(rewards) if len(rewards)>0 else 0.0\n",
    "    target = 0.8 * max_reward\n",
    "    episodes_to_80 = next((idx for idx, r in enumerate(ma) if r >= target), None) if len(ma)>0 else None\n",
    "    print(f\"Seed {seeds[i]} | Episodes to 80% max reward: {episodes_to_80}\")\n",
    "    if len(ma)>0:\n",
    "        plt.plot(ma, label=f\"Seed {seeds[i]} (MA 10)\")\n",
    "plt.title(\"DQN: Moving Average Reward per Seed (Sample Efficiency)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a34789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 7. Convergence & Stability Visualization\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12,5))\n",
    "for i, rewards in enumerate(all_rewards):\n",
    "    ma = np.convolve(rewards, np.ones(ma_window)/ma_window, mode='valid') if len(rewards)>=ma_window else np.array([])\n",
    "    if len(ma)>0:\n",
    "        plt.plot(ma, label=f\"Seed {seeds[i]}\")\n",
    "plt.title(\"Convergence Across Seeds (Learning Stability)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Moving Average Reward\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Variance across seeds\n",
    "max_len = max(len(r) for r in all_rewards) if len(all_rewards)>0 else 0\n",
    "padded_rewards = [np.pad(r, (0, max_len - len(r)), 'edge') for r in all_rewards] if max_len>0 else []\n",
    "reward_variance_across_seeds = np.var(padded_rewards, axis=0) if len(padded_rewards)>0 else np.array([])\n",
    "plt.figure(figsize=(12,4))\n",
    "if reward_variance_across_seeds.size>0:\n",
    "    plt.plot(reward_variance_across_seeds)\n",
    "plt.title(\"Reward Variance Across Seeds (Learning Stability)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 8. Exploration Stability\n",
    "# -----------------------\n",
    "# Epsilon decay\n",
    "plt.figure(figsize=(12,4))\n",
    "for i, eps in enumerate(all_epsilons):\n",
    "    plt.plot(eps, label=f\"Seed {seeds[i]}\")\n",
    "plt.title(\"Epsilon Decay over Time\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Reward variance per 10-episode block\n",
    "block_size = 10\n",
    "plt.figure(figsize=(12,4))\n",
    "for i, rewards in enumerate(all_rewards):\n",
    "    variance = [np.var(rewards[j:j+block_size]) for j in range(0, len(rewards), block_size)]\n",
    "    plt.plot(variance, label=f\"Seed {seeds[i]}\")\n",
    "plt.title(\"Reward Variance per 10 Episodes\")\n",
    "plt.xlabel(\"Block\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 9. Policy Behavior (Steps Survived)\n",
    "# -----------------------\n",
    "plt.figure(figsize=(12,4))\n",
    "for i, steps in enumerate(all_steps):\n",
    "    plt.plot(steps, label=f\"Seed {seeds[i]}\")\n",
    "plt.title(\"Steps Survived per Episode (Policy Behavior)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Steps\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc492409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 10. Runtime / Computational Efficiency\n",
    "# -----------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([str(s) for s in seeds], runtime_per_seed)\n",
    "plt.title(\"Average Time per Episode per Seed (Runtime)\")\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Seconds per Episode\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c85cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 11. Gameplay Recording: Best-Performing Episode\n",
    "# -----------------------\n",
    "# Find best episode\n",
    "best_seed_idx = None\n",
    "best_episode_idx = None\n",
    "best_reward = -np.inf\n",
    "\n",
    "for i, rewards in enumerate(all_rewards):\n",
    "    if len(rewards)==0:\n",
    "        continue\n",
    "    ep_idx = int(np.argmax(rewards))\n",
    "    if rewards[ep_idx] > best_reward:\n",
    "        best_reward = rewards[ep_idx]\n",
    "        best_seed_idx = i\n",
    "        best_episode_idx = ep_idx\n",
    "\n",
    "if best_seed_idx is None:\n",
    "    print(\"No best episode found.\")\n",
    "else:\n",
    "    best_seed_val = seeds[best_seed_idx]\n",
    "    print(f\"Best episode: Seed {best_seed_val}, Episode {best_episode_idx}, Reward {best_reward:.2f}\")\n",
    "\n",
    "    # Load corresponding model for deterministic replay\n",
    "    model_file = f\"dqn_model_seed_{best_seed_val}.pth\"\n",
    "    play_model = DQNNet(input_dim=3, hidden=hidden_size, output_dim=n_actions).to(device)\n",
    "    if os.path.exists(model_file):\n",
    "        play_model.load_state_dict(torch.load(model_file, map_location=device))\n",
    "    play_model.eval()\n",
    "\n",
    "    env.reset()\n",
    "    state = get_state(env)\n",
    "    state_arr = state_to_array(state)\n",
    "    done = False\n",
    "    frames = []\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            qvals = play_model(torch.tensor(state_arr, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "            action = int(torch.argmax(qvals).item())\n",
    "        frame, reward, done, _ = env.step(action)\n",
    "        frames.append(frame)\n",
    "        state = get_state(env)\n",
    "        state_arr = state_to_array(state)\n",
    "\n",
    "    print(f\"Recorded {len(frames)} steps for the best episode.\")\n",
    "\n",
    "    # Save frames as video (unique filename)\n",
    "    if len(frames) == 0:\n",
    "        print(\"No frames to save.\")\n",
    "    elif not _HAS_CV2:\n",
    "        print(\"OpenCV not available — frames captured but not saved as video. Install opencv-python to enable saving.\")\n",
    "    else:\n",
    "        ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        rounded_r = int(round(best_reward))\n",
    "        video_filename = f\"best_snake_dqn_seed{best_seed_val}_ep{best_episode_idx}_r{rounded_r}_{ts}.avi\"\n",
    "        height, width, _ = frames[0].shape\n",
    "        out = cv2.VideoWriter(video_filename, cv2.VideoWriter_fourcc(*'XVID'), 15, (width, height))\n",
    "        for f in frames:\n",
    "            if f.shape[2] == 4:\n",
    "                f_bgr = cv2.cvtColor(f, cv2.COLOR_RGBA2BGR)\n",
    "            else:\n",
    "                f_bgr = f\n",
    "            out.write(f_bgr)\n",
    "        out.release()\n",
    "        print(f\"Video saved as '{video_filename}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c82346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 12. Generate HTML Summary Report\n",
    "# -----------------------\n",
    "html_report = \"snake_dqn_summary.html\"\n",
    "doc, tag, text, line = Doc().ttl()\n",
    "\n",
    "with tag('html'):\n",
    "    with tag('head'):\n",
    "        line('title', 'Snake DQN Summary Report')\n",
    "    with tag('body'):\n",
    "        line('h1', 'Snake DQN Summary Report')\n",
    "        \n",
    "        with tag('h2'):\n",
    "            text(\"Per-Seed Metrics\")\n",
    "        \n",
    "        # Table header\n",
    "        with tag('table', border=\"1\", style=\"border-collapse:collapse; width:80%\"):\n",
    "            with tag('tr'):\n",
    "                for col in [\"Seed\", \"Episodes to 80% Max Reward\", \"Mean Reward\", \"Std Reward\",\n",
    "                            \"Final Epsilon\", \"Avg Steps Survived\", \"Avg Runtime/Episode (s)\"]:\n",
    "                    line('th', col, style=\"padding:5px; text-align:center;\")\n",
    "            \n",
    "            # Table rows\n",
    "            for i, rewards in enumerate(all_rewards):\n",
    "                ma = np.convolve(rewards, np.ones(ma_window)/ma_window, mode='valid') if len(rewards)>=ma_window else np.array([])\n",
    "                max_reward = max(rewards) if len(rewards)>0 else 0.0\n",
    "                target = 0.8 * max_reward\n",
    "                episodes_to_80 = next((idx for idx, r in enumerate(ma) if r >= target), None) if len(ma)>0 else None\n",
    "                \n",
    "                mean_reward = np.mean(rewards) if len(rewards)>0 else 0.0\n",
    "                std_reward = np.std(rewards) if len(rewards)>0 else 0.0\n",
    "                final_epsilon = all_epsilons[i][-1] if len(all_epsilons[i])>0 else 0.0\n",
    "                avg_steps = np.mean(all_steps[i]) if len(all_steps[i])>0 else 0.0\n",
    "                avg_runtime = runtime_per_seed[i] if i < len(runtime_per_seed) else 0.0\n",
    "                \n",
    "                with tag('tr'):\n",
    "                    for val in [seeds[i], episodes_to_80 if episodes_to_80 is not None else \"N/A\",\n",
    "                                f\"{mean_reward:.2f}\", f\"{std_reward:.2f}\",\n",
    "                                f\"{final_epsilon:.3f}\", f\"{avg_steps:.2f}\", f\"{avg_runtime:.3f}\"]:\n",
    "                        line('td', str(val), style=\"padding:5px; text-align:center;\")\n",
    "        \n",
    "        # Best episode info\n",
    "        with tag('h2'):\n",
    "            text(\"Best Episode Across All Seeds\")\n",
    "        if best_seed_idx is not None:\n",
    "            line('p', f\"Seed {seeds[best_seed_idx]}, Episode {best_episode_idx}, Reward {best_reward:.2f}\")\n",
    "            if 'video_filename' in locals() and video_filename is not None:\n",
    "                line('p', f\"Gameplay video saved as '{video_filename}'\")\n",
    "            else:\n",
    "                line('p', \"Gameplay video not saved (either no frames or OpenCV missing).\")\n",
    "        else:\n",
    "            line('p', \"No best episode found.\")\n",
    "        \n",
    "# Save HTML report\n",
    "with open(html_report, \"w\") as f:\n",
    "    f.write(indent(doc.getvalue()))\n",
    "\n",
    "print(f\"✅ HTML summary report saved as '{html_report}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
